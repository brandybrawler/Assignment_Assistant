To run this ,install allthe requirements in this requirements.txt file,then run the python file either with 
"python3 llm.py" or "python llm.py"

This will load the llm model into memory,this process takes a while and may seem like it is not working,just wait.
When the model is loaded into memory,flask server will  start up and then you can chat with your model using the web application from the main project.